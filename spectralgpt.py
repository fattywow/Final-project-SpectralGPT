# -*- coding: utf-8 -*-
"""SpectralGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15k6ijTOt2Qu_vUNC5IDMeLMB_svBrlTq
"""

!apt-get install -y p7zip-full

!7z x /content/img.7z -o/content/SegMunich/img
!7z x /content/label.7z -o/content/SegMunich/label

# âœ… SegMunich Auto-Pairing Script for Semantic Change Detection
# ğŸ“ Designed for Colab, input = 10-band images + labels, output = .npz with T1/T2/labels/change_map

import os
import numpy as np
from tifffile import imread
from tqdm import tqdm
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import shutil

# === CONFIG ===
IMG_DIR = '/content/SegMunich/img/img'
LABEL_DIR = '/content/SegMunich/label/label'
OUTPUT_DIR = '/content/paired_data'
N_PAIRS = 2000
REQUIRED_BANDS = 10
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === STEP 1: Load and extract spatial-aware spectral vectors ===
print("[1/5] Reading spatial features...")
file_list = sorted([f for f in os.listdir(IMG_DIR) if f.endswith('.tif')])
features = []
valid_files = []

for fname in tqdm(file_list):
    try:
        img = imread(os.path.join(IMG_DIR, fname))
        print(f"{fname} shape: {img.shape}")

        # Convert to [C, H, W]
        if img.ndim == 3:
            if img.shape[0] == REQUIRED_BANDS:
                pass
            elif img.shape[2] == REQUIRED_BANDS:
                img = img.transpose(2, 0, 1)
            else:
                print(f"Skipped {fname}: unexpected shape {img.shape}")
                continue
        else:
            print(f"Skipped {fname}: not a 3D image")
            continue

        # Apply PCA to full image pixels [C, H, W] -> [H*W, C]
        flat_img = img.transpose(1, 2, 0).reshape(-1, REQUIRED_BANDS)
        spatial_feat = PCA(n_components=3).fit_transform(flat_img).reshape(128, 128, 3)
        feature_vector = spatial_feat.flatten()  # [128*128*3]

        features.append(feature_vector)
        valid_files.append(fname)
    except Exception as e:
        print(f"Error reading {fname}: {e}")
        continue

if len(features) == 0:
    raise ValueError("âŒ No valid image files with required band count were found.")

features = np.stack(features)

# === STEP 2: PCA for final reduction (optional) ===
print("[2/5] Applying PCA...")
pca = PCA(n_components=64)  # Reduce high-dimensional flattened vectors
reduced = pca.fit_transform(features)

# === STEP 3: Global top-N pairing based on cosine similarity ===
print("[3/5] Finding top-N similar image pairs...")
sim_matrix = cosine_similarity(reduced)
np.fill_diagonal(sim_matrix, -1.0)

# Extract upper triangle indices (i < j)
i_idx, j_idx = np.triu_indices(sim_matrix.shape[0], k=1)
sim_scores = sim_matrix[i_idx, j_idx]

# Sort and select top-N
sorted_indices = np.argsort(sim_scores)[::-1]
top_pairs = []
used = set()

for idx in sorted_indices:
    i, j = i_idx[idx], j_idx[idx]
    if valid_files[i] in used or valid_files[j] in used:
        continue
    top_pairs.append((valid_files[i], valid_files[j]))
    used.add(valid_files[i])
    used.add(valid_files[j])
    if len(top_pairs) >= N_PAIRS:
        break

# === STEP 4: Load paired data and generate change maps ===
print("[4/5] Generating .npz files with change maps...")
for idx, (f1, f2) in enumerate(tqdm(top_pairs)):
    img1 = imread(os.path.join(IMG_DIR, f1))
    img2 = imread(os.path.join(IMG_DIR, f2))

    if img1.ndim == 3 and img1.shape[2] == REQUIRED_BANDS:
        img1 = img1.transpose(2, 0, 1)
    if img2.ndim == 3 and img2.shape[2] == REQUIRED_BANDS:
        img2 = img2.transpose(2, 0, 1)

    lab1 = imread(os.path.join(LABEL_DIR, f1))
    lab2 = imread(os.path.join(LABEL_DIR, f2))

    change_map = (lab1 != lab2).astype(np.uint8)

    outpath = os.path.join(OUTPUT_DIR, f'pair_{idx:04d}.npz')
    np.savez_compressed(outpath,
                        T1=img1, T2=img2,
                        T1_label=lab1, T2_label=lab2,
                        change_map=change_map)

# === DONE ===
print(f"[5/5] âœ… Finished. Saved {len(top_pairs)} paired .npz files to {OUTPUT_DIR}")

import os
import numpy as np

npz_dir = '/content/paired_data'
npz_files = sorted([f for f in os.listdir(npz_dir) if f.endswith('.npz')])

label_set = set()

for file in npz_files:
    data = np.load(os.path.join(npz_dir, file))
    t1_labels = np.unique(data['T1_label'])
    t2_labels = np.unique(data['T2_label'])

    label_set.update(t1_labels.tolist())
    label_set.update(t2_labels.tolist())

sorted_labels = sorted(label_set)
print(f"âœ… ç¸½å…±æœ‰ {len(sorted_labels)} ç¨®èªæ„é¡åˆ¥ï¼š{sorted_labels}")

import os
import numpy as np
import json

npz_dir = '/content/paired_data'
npz_files = sorted([f for f in os.listdir(npz_dir) if f.endswith('.npz')])

# === 1. æƒææ‰€æœ‰å‡ºç¾éçš„èªæ„é¡åˆ¥ ===
label_set = set()
for file in npz_files:
    data = np.load(os.path.join(npz_dir, file))
    label_set.update(np.unique(data['T1_label']).tolist())
    label_set.update(np.unique(data['T2_label']).tolist())

unique_labels = sorted(label_set)
print(f"âœ… æ‰¾åˆ° {len(unique_labels)} ç¨®èªæ„é¡åˆ¥ï¼š{unique_labels}")

label_to_index = {label: idx for idx, label in enumerate(unique_labels)}
index_to_label = {idx: label for idx, label in enumerate(unique_labels)}
num_classes = len(unique_labels)

# å„²å­˜å°ç…§è¡¨ï¼ˆå¯é¸ï¼‰
save_dir = os.path.join(npz_dir, 'with_transition')
os.makedirs(save_dir, exist_ok=True)
with open(os.path.join(save_dir, 'label_mapping.json'), 'w') as f:
    json.dump(label_to_index, f, indent=2)

# === 2. å»ºç«‹ transition_label ä¸¦å„²å­˜ç‚ºæ–°æª” ===
for file in npz_files:
    path = os.path.join(npz_dir, file)
    data = np.load(path)

    T1_label = data['T1_label']
    T2_label = data['T2_label']

    # è½‰æ›æˆ index
    T1_index = np.vectorize(label_to_index.get)(T1_label)
    T2_index = np.vectorize(label_to_index.get)(T2_label)

    # è¨ˆç®— transition label
    transition_label = T1_index * num_classes + T2_index
    transition_label = transition_label.astype(np.uint16)

    # é¡¯ç¤ºå‡ºç¾éçš„è½‰æ›é¡å‹ï¼ˆè½‰å›åŸå§‹ labelï¼‰
    transition_pairs = set(zip(T1_index.flatten(), T2_index.flatten()))
    readable_pairs = sorted([(index_to_label[i], index_to_label[j]) for i, j in transition_pairs])

    # å„²å­˜æ‰€æœ‰åŸå§‹è³‡æ–™ + transition_label
    save_path = os.path.join(save_dir, file)
    np.savez_compressed(save_path,
                        T1=data['T1'],
                        T2=data['T2'],
                        T1_label=T1_label,
                        T2_label=T2_label,
                        change_map=data['change_map'],
                        transition_label=transition_label)
    print(f"âœ… å·²å„²å­˜å« transition_label çš„æª”æ¡ˆï¼š{file}")

print(f"\nğŸ‰ å…¨éƒ¨è™•ç†å®Œæˆï¼Œæª”æ¡ˆå·²å­˜å…¥ {save_dir}")

import numpy as np

# æŒ‡å®šæª”æ¡ˆè·¯å¾‘
file_path = "/content/paired_data/with_transition/pair_0000.npz"

# è¼‰å…¥ .npz æª”æ¡ˆ
data = np.load(file_path)

# å°å‡ºæ¯å€‹ key çš„ shape å’Œ dtype
print(f"ğŸ“¦ æª”æ¡ˆåŒ…å« {len(data.files)} å€‹é …ç›®ï¼š")
for key in data.files:
    print(f"  {key}: shape={data[key].shape}, dtype={data[key].dtype}")

# === Semantic Change Detection: 9:1 Split and Training ===

import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# === Dataset Class ===
class PairedChangeDataset(tf.keras.utils.Sequence):
    def __init__(self, file_list, npz_dir, batch_size=8):
        self.files = file_list
        self.npz_dir = npz_dir
        self.batch_size = batch_size

    def __len__(self):
        return len(self.files) // self.batch_size

    def __getitem__(self, idx):
        batch_files = self.files[idx * self.batch_size:(idx + 1) * self.batch_size]
        T1_batch, T2_batch, label_batch = [], [], []
        for f in batch_files:
            data = np.load(os.path.join(self.npz_dir, f))
            T1, T2 = data['T1'], data['T2']
            label = data['transition_label']
            T1_batch.append(T1)
            T2_batch.append(T2)
            label_batch.append(label)

        T1 = np.array(T1_batch).astype(np.float32).transpose(0, 2, 3, 1)
        T2 = np.array(T2_batch).astype(np.float32).transpose(0, 2, 3, 1)
        label = np.array(label_batch).astype(np.int32)
        return ((T1, T2), label)

# === Model ===
def conv_block(x, filters):
    x = tf.keras.layers.Conv2D(filters, 3, padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    return x

# âœ… å®‰å…¨å–ä»£ Lambda çš„ tf.abs
class AbsLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        return tf.math.abs(inputs)

def build_model(input_shape=(128, 128, 10)):
    t1_input = tf.keras.Input(shape=input_shape)
    t2_input = tf.keras.Input(shape=input_shape)

    def encoder(x):
        x = conv_block(x, 64)
        x = conv_block(x, 128)
        x = conv_block(x, 256)
        return x  # shape still (128, 128, 256)

    x1 = encoder(t1_input)
    x2 = encoder(t2_input)

    diff = tf.keras.layers.Subtract()([x1, x2])
    diff = AbsLayer()(diff)

    x = conv_block(diff, 128)
    x = conv_block(x, 64)
    output = tf.keras.layers.Conv2D(169, 1, activation='softmax')(x)

    model = tf.keras.Model(inputs=[t1_input, t2_input], outputs=output)
    return model

# === Training ===
if __name__ == '__main__':
    npz_dir = '/content/paired_data/with_transition'
    all_files = sorted([f for f in os.listdir(npz_dir) if f.endswith('.npz')])
    train_files, test_files = train_test_split(all_files, test_size=0.1, random_state=42)

    train_dataset = PairedChangeDataset(train_files, npz_dir, batch_size=8)
    test_dataset = PairedChangeDataset(test_files, npz_dir, batch_size=8)

    print("ğŸš€ GPUs:", tf.config.list_physical_devices('GPU'))

    model = build_model()
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-3),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
        metrics=['sparse_categorical_accuracy']
    )

    print("ğŸš€ Training started")
    model.fit(train_dataset, epochs=10)

    # âœ… å„²å­˜æ¨¡å‹æ™‚åŒ…å«è‡ªè¨‚ Layer
    model.save('change_model_softmax.h5')
    print("âœ… Model saved.")

# === Inference + Accuracy å¯è¦–åŒ–ï¼ˆTrainingï¼‰===
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from collections import Counter

# âœ… Colormap for 13 é¡
color_list = [
    "#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
    "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf",
    "#f781bf", "#a65628", "#4daf4a"
]
custom_cmap = mcolors.ListedColormap(color_list)

# âœ… ä½¿ç”¨ç¬¬ä¸€å¼µ training sample
(T1_train_batch, T2_train_batch), label_train_batch = train_dataset[0]
T1 = T1_train_batch[0]
T2 = T2_train_batch[0]
true_label = label_train_batch[0]

# âœ… é æ¸¬
pred_logits = model.predict([np.expand_dims(T1, 0), np.expand_dims(T2, 0)])
pred_label = np.argmax(pred_logits[0], axis=-1)

# âœ… æ‹†æˆ T1 / T2
true_T1 = true_label // 13
true_T2 = true_label % 13
pred_T1 = pred_label // 13
pred_T2 = pred_label % 13

# âœ… Accuracy è¨ˆç®—
transition_acc = np.mean(pred_label == true_label)
t1_acc = np.mean(pred_T1 == true_T1)
t2_acc = np.mean(pred_T2 == true_T2)

print(f"ğŸ¯ Transition Accuracy: {transition_acc:.4f}  (T1 & T2 both correct)")
print(f"ğŸ”¹ T1 Accuracy:         {t1_acc:.4f}")
print(f"ğŸ”¸ T2 Accuracy:         {t2_acc:.4f}")
print("Predicted T1 distribution:", Counter(pred_T1.flatten()))
print("Predicted T2 distribution:", Counter(pred_T2.flatten()))

# âœ… å¯è¦–åŒ– Transition, T1, T2
fig, axs = plt.subplots(2, 2, figsize=(10, 14))

# T1
axs[0, 0].imshow(true_T1, cmap=custom_cmap, vmin=0, vmax=12)
axs[0, 0].set_title("True T1 (Fixed Colors)")
axs[0, 1].imshow(pred_T1, cmap=custom_cmap, vmin=0, vmax=12)
axs[0, 1].set_title("Predicted T1 (Fixed Colors)")

# T2
axs[1, 0].imshow(true_T2, cmap=custom_cmap, vmin=0, vmax=12)
axs[1, 0].set_title("True T2 (Fixed Colors)")
axs[1, 1].imshow(pred_T2, cmap=custom_cmap, vmin=0, vmax=12)
axs[1, 1].set_title("Predicted T2 (Fixed Colors)")

for ax in axs.ravel():
    ax.axis('off')
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import tensorflow as tf
import os
from sklearn.model_selection import train_test_split
from collections import Counter

# âœ… è‡ªå®š Layer for loading
class AbsLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        return tf.math.abs(inputs)

# âœ… Dataset
class PairedChangeDataset(tf.keras.utils.Sequence):
    def __init__(self, file_list, npz_dir, batch_size=8):
        self.files = file_list
        self.npz_dir = npz_dir
        self.batch_size = batch_size

    def __len__(self):
        return len(self.files) // self.batch_size

    def __getitem__(self, idx):
        batch_files = self.files[idx * self.batch_size:(idx + 1) * self.batch_size]
        T1_batch, T2_batch, label_batch = [], [], []
        for f in batch_files:
            data = np.load(os.path.join(self.npz_dir, f))
            T1, T2 = data['T1'], data['T2']
            label = data['transition_label']
            T1_batch.append(T1)
            T2_batch.append(T2)
            label_batch.append(label)

        T1 = np.array(T1_batch).astype(np.float32).transpose(0, 2, 3, 1)
        T2 = np.array(T2_batch).astype(np.float32).transpose(0, 2, 3, 1)
        label = np.array(label_batch).astype(np.int32)
        return ((T1, T2), label)

# âœ… å›ºå®š colormapï¼ˆ13 é¡ï¼‰
color_list = [
    "#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
    "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf",
    "#f781bf", "#a65628", "#4daf4a"
]
custom_cmap = mcolors.ListedColormap(color_list)

# === Block B: Visualize First Testing Sample ===
if __name__ == '__main__':
    npz_dir = '/content/paired_data/with_transition'
    all_files = sorted([f for f in os.listdir(npz_dir) if f.endswith('.npz')])
    _, test_files = train_test_split(all_files, test_size=0.1, random_state=42)

    test_dataset = PairedChangeDataset(test_files, npz_dir, batch_size=8)
    model = tf.keras.models.load_model('change_model_softmax.h5', custom_objects={'AbsLayer': AbsLayer})

    (T1_test_batch, T2_test_batch), label_test_batch = test_dataset[0]
    T1 = T1_test_batch[0]
    T2 = T2_test_batch[0]
    true_label = label_test_batch[0]

    pred_logits = model.predict([np.expand_dims(T1, 0), np.expand_dims(T2, 0)])
    pred_label = np.argmax(pred_logits[0], axis=-1)

    # æ‹†æˆ T1 å’Œ T2
    true_T1 = true_label // 13
    true_T2 = true_label % 13
    pred_T1 = pred_label // 13
    pred_T2 = pred_label % 13

    # === Accuracy
    transition_acc = np.mean(pred_label == true_label)
    t1_acc = np.mean(pred_T1 == true_T1)
    t2_acc = np.mean(pred_T2 == true_T2)

    print(f"ğŸ¯ Transition Accuracy: {transition_acc:.4f}  (T1 & T2 both correct)")
    print(f"ğŸ”¹ T1 Accuracy:         {t1_acc:.4f}")
    print(f"ğŸ”¸ T2 Accuracy:         {t2_acc:.4f}")
    print("Predicted T1 distribution:", Counter(pred_T1.flatten()))
    print("Predicted T2 distribution:", Counter(pred_T2.flatten()))

    # === å¯è¦–åŒ–ï¼šTransition + T1 + T2
    fig, axs = plt.subplots(2, 2, figsize=(10, 14))

    # T1
    axs[0, 0].imshow(true_T1, cmap=custom_cmap, vmin=0, vmax=12)
    axs[0, 0].set_title("True T1 (Fixed Colors)")
    axs[0, 1].imshow(pred_T1, cmap=custom_cmap, vmin=0, vmax=12)
    axs[0, 1].set_title("Predicted T1 (Fixed Colors)")

    # T2
    axs[1, 0].imshow(true_T2, cmap=custom_cmap, vmin=0, vmax=12)
    axs[1, 0].set_title("True T2 (Fixed Colors)")
    axs[1, 1].imshow(pred_T2, cmap=custom_cmap, vmin=0, vmax=12)
    axs[1, 1].set_title("Predicted T2 (Fixed Colors)")

    for ax in axs.ravel():
        ax.axis('off')

    plt.tight_layout()
    plt.show()